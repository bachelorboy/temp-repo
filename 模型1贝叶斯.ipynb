{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-04T04:25:01.957777Z",
     "start_time": "2026-01-04T04:05:46.882567Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, max_error\n",
    "import optuna\n",
    "import copy\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "# 忽略一些Pandas的警告\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置matplotlib\n",
    "try:\n",
    "    matplotlib.use('TkAgg')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# ==========================================\n",
    "# 0. 全局配置\n",
    "# ==========================================\n",
    "GLOBAL_CONFIG = {\n",
    "    # === [源域配置] 直接指定两个源域文件的绝对路径 ===\n",
    "    'source_files': [\n",
    "        r\"C:\\Users\\CHX\\xwechat_files\\wxid_uomr32ya9e9k11_d15b\\msg\\file\\2026-01\\Filtered_Dataset_1_NCA_battery_features_wide.csv\",\n",
    "        r\"C:\\Users\\CHX\\xwechat_files\\wxid_uomr32ya9e9k11_d15b\\msg\\file\\2026-01\\Filtered_Dataset_2_NCM_battery_features_wide.csv\",\n",
    "        r\"C:\\Users\\CHX\\xwechat_files\\wxid_uomr32ya9e9k11_d15b\\msg\\file\\2026-01\\Filtered_Dataset_3_NCM_NCA_battery_features_wide_downsampled_120s.csv\"\n",
    "    ],\n",
    "    # 'source_files': [\n",
    "    #     r\"C:\\Users\\CHX\\xwechat_files\\wxid_uomr32ya9e9k11_d15b\\msg\\file\\2026-01\\Dataset_2_NCM_battery_features_wide.csv\",\n",
    "    #     r\"C:\\Users\\CHX\\xwechat_files\\wxid_uomr32ya9e9k11_d15b\\msg\\file\\2026-01\\Dataset_3_NCM_NCA_battery_features_wide_downsampled_120s.csv\"\n",
    "    # ],\n",
    "    # === [目标域配置] 目标域文件所在的父文件夹 ===\n",
    "    'target_parent_folder': r\"C:\\Users\\CHX\\xwechat_files\\wxid_uomr32ya9e9k11_d15b\\msg\\file\\2026-01\\降采样耐久性测试数据\",\n",
    "    \n",
    "    # === [结果保存] ===\n",
    "    'save_dir': r\"C:\\Users\\CHX\\xwechat_files\\wxid_uomr32ya9e9k11_d15b\\msg\\file\\2026-01\\2409贝叶斯迁移结果\\LFP降采样SOH\", \n",
    "    \n",
    "    # === [目标域划分] 4个微调(Train)，4个测试(Test) ===\n",
    "    # 注意：这里只需要文件名的关键词即可，不用写全路径\n",
    "    'train_car_names': ['battery_1', 'battery_5', 'battery_8'], \n",
    "    'test_car_names':  ['battery_2', 'battery_4', 'battery_3', 'battery_6', 'battery_7'],\n",
    "    \n",
    "    # === [特征列] 14个电压片段 ===\n",
    "    'features': [f'V_sec_{i}' for i in range(1, 15)], # V_sec_1 到 V_sec_14\n",
    "    \n",
    "    'label_col': 'SOH',       # 标签列名\n",
    "    'time_col': 'start_time', # 这里的CSV可能不一定有时间列，代码里会做兼容处理(用索引)\n",
    "    'input_dim': 14,          # 输入维度\n",
    "    'latent_dim': 6,          # 隐层维度 (因为输入变多了，稍微增加一点中间层宽度)\n",
    "    \n",
    "    'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    'n_trials': 80,       # 贝叶斯优化次数\n",
    "    'max_epochs': 1000,   # 最大训练轮数\n",
    "    'patience': 200       # 早停耐心值\n",
    "}\n",
    "\n",
    "# 确保保存目录存在\n",
    "os.makedirs(GLOBAL_CONFIG['save_dir'], exist_ok=True)\n",
    "print(f\"Using device: {GLOBAL_CONFIG['device']}\")\n",
    "print(f\"Results will be saved to: {GLOBAL_CONFIG['save_dir']}\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. 工具类与数据加载\n",
    "# ==========================================\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=200, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = copy.deepcopy(model.state_dict())\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = copy.deepcopy(model.state_dict())\n",
    "            self.counter = 0\n",
    "\n",
    "def load_source_data_csv(file_list, feature_cols, label_col):\n",
    "    \"\"\"加载源域数据 (CSV格式)\"\"\"\n",
    "    data_list = []\n",
    "    print(f\"Loading Source Data from {len(file_list)} specific files...\")\n",
    "    for f in file_list:\n",
    "        if not os.path.exists(f):\n",
    "            print(f\"[Warning] Source file not found: {f}\")\n",
    "            continue\n",
    "        try:\n",
    "            df = pd.read_csv(f)\n",
    "            # 确保没有空值\n",
    "            df = df.dropna(subset=feature_cols + [label_col])\n",
    "            features = df[feature_cols].values\n",
    "            # SOH 已经是 0.x 的小数，无需额外处理\n",
    "            labels = df[label_col].values.reshape(-1, 1) \n",
    "            data_list.append(np.hstack((features, labels)))\n",
    "            print(f\"  Loaded {os.path.basename(f)}: shape {features.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error reading {os.path.basename(f)}: {e}\")\n",
    "\n",
    "    if not data_list: raise ValueError(\"No source data loaded.\")\n",
    "    combined = np.vstack(data_list)\n",
    "    return combined[:, :-1], combined[:, -1:]\n",
    "\n",
    "def get_files_by_names(folder_path, target_names):\n",
    "    \"\"\"辅助函数：根据名字筛选CSV文件\"\"\"\n",
    "    all_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "    selected_files = []\n",
    "    for f in all_files:\n",
    "        fname = os.path.basename(f)\n",
    "        for target in target_names:\n",
    "            # 简单的关键词匹配\n",
    "            if target in fname:\n",
    "                selected_files.append(f)\n",
    "                break \n",
    "    return sorted(selected_files)\n",
    "\n",
    "def load_target_train_stacked_csv(folder_path, target_names, feature_cols, label_col):\n",
    "    \"\"\"加载目标域训练集 (CSV)\"\"\"\n",
    "    files = get_files_by_names(folder_path, target_names)\n",
    "    print(f\"Loading Target Train Data: Found {len(files)} files matching {target_names}\")\n",
    "    \n",
    "    data_list = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            df = pd.read_csv(f)\n",
    "            df = df.dropna(subset=feature_cols + [label_col])\n",
    "            features = df[feature_cols].values\n",
    "            labels = df[label_col].values.reshape(-1, 1)\n",
    "            data_list.append(np.hstack((features, labels)))\n",
    "            print(f\"  Loaded Train File: {os.path.basename(f)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error reading {os.path.basename(f)}: {e}\")\n",
    "            \n",
    "    if not data_list: raise ValueError(f\"No target train data found for names: {target_names}\")\n",
    "    combined = np.vstack(data_list)\n",
    "    return combined[:, :-1], combined[:, -1:]\n",
    "\n",
    "def load_target_test_individual_csv(folder_path, target_names, feature_cols, label_col):\n",
    "    \"\"\"加载目标域测试集 (CSV)：单独保存用于评估\"\"\"\n",
    "    files = get_files_by_names(folder_path, target_names)\n",
    "    print(f\"Loading Target Test Data: Found {len(files)} files matching {target_names}\")\n",
    "    \n",
    "    cars_data = [] \n",
    "    for f in files:\n",
    "        try:\n",
    "            df = pd.read_csv(f)\n",
    "            # 处理时间列：如果有 'start_time' 或其他列可以使用，否则使用索引\n",
    "            time_feat = GLOBAL_CONFIG['time_col']\n",
    "            if time_feat not in df.columns:\n",
    "                # 尝试寻找常见的时间列名，如果都没有则用Index\n",
    "                times = df.index.values\n",
    "            else:\n",
    "                times = df[time_feat].values\n",
    "            \n",
    "            df = df.dropna(subset=feature_cols + [label_col])\n",
    "            \n",
    "            X_raw = df[feature_cols].values\n",
    "            y_raw = df[label_col].values.reshape(-1, 1)\n",
    "            # 重新对齐时间（因为dropna可能删了行）\n",
    "            # 注意：如果时间列很重要且必须对齐，dropna这里要小心。这里为了demo简单，假设dropna删的不多\n",
    "            # 如果dropna后长度对不上，我们截断times\n",
    "            if len(times) != len(X_raw):\n",
    "                 times = times[:len(X_raw)]\n",
    "\n",
    "            filename = os.path.basename(f)\n",
    "            \n",
    "            cars_data.append({\n",
    "                'X_raw': X_raw, \n",
    "                'y_raw': y_raw, \n",
    "                'times': times,\n",
    "                'name': filename\n",
    "            })\n",
    "            print(f\"  Loaded Test File: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error reading test file {os.path.basename(f)}: {e}\")\n",
    "            \n",
    "    return cars_data\n",
    "def get_dataloader(X, y, batch_size, shuffle=True, drop_last=False):\n",
    "    tensor_x = torch.FloatTensor(X)\n",
    "    tensor_y = torch.FloatTensor(y)\n",
    "    dataset = TensorDataset(tensor_x, tensor_y)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 2. 模型定义 (对应输入维度14)\n",
    "# ==========================================\n",
    "class SAE_FCN_Paper(nn.Module):\n",
    "    def __init__(self, input_dim=14, latent_dim=6):\n",
    "        super(SAE_FCN_Paper, self).__init__()\n",
    "        # 编码器：14 -> 24 -> 12 -> latent_dim\n",
    "        self.enc1 = nn.Sequential(nn.Linear(input_dim, 24), nn.BatchNorm1d(24), nn.ReLU(), nn.Linear(24, 12), nn.Tanh())\n",
    "        self.enc2 = nn.Sequential(nn.Linear(12, 8), nn.BatchNorm1d(8), nn.ReLU(), nn.Linear(8, latent_dim), nn.Tanh())\n",
    "        \n",
    "        # 解码器\n",
    "        self.dec2 = nn.Sequential(nn.Linear(latent_dim, 8), nn.BatchNorm1d(8), nn.Sigmoid(), nn.Linear(8, 12))\n",
    "        self.dec1 = nn.Sequential(nn.Linear(12, 24), nn.BatchNorm1d(24), nn.Sigmoid(), nn.Linear(24, input_dim))\n",
    "        \n",
    "        # FCN回归器\n",
    "        self.fcn = nn.Sequential(nn.Linear(latent_dim, 16), nn.ReLU(), nn.Linear(16, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat_12 = self.enc1(x)\n",
    "        features = self.enc2(feat_12)\n",
    "        rec_12 = self.dec2(features)\n",
    "        x_recon = self.dec1(rec_12)\n",
    "        soh_pred = self.fcn(features)\n",
    "        return x_recon, soh_pred, features\n",
    "\n",
    "# ==========================================\n",
    "# 3. 贝叶斯优化 Objective\n",
    "# ==========================================\n",
    "def objective(trial, X_src, y_src, X_tgt_train, y_tgt_train, test_cars_data, scaler_tgt):\n",
    "    \n",
    "    # 3.1 采样超参数\n",
    "    lr_pre = trial.suggest_loguniform('lr_pretrain', 1e-4, 1e-2)\n",
    "    lr_fine = trial.suggest_loguniform('lr_finetune', 1e-5, 1e-3)\n",
    "    lambda_w = trial.suggest_float('lambda_weight', 0.1, 0.9)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    \n",
    "    # 3.2 数据准备\n",
    "    loader_src = get_dataloader(X_src, y_src, batch_size, shuffle=True, drop_last=True)\n",
    "    loader_tgt_tr = get_dataloader(X_tgt_train, y_tgt_train, batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    # 构建 Validation Set (所有测试车数据合并，只为计算Pre-Pruning Loss)\n",
    "    X_test_all_raw = np.vstack([c['X_raw'] for c in test_cars_data])\n",
    "    y_test_all_raw = np.vstack([c['y_raw'] for c in test_cars_data])\n",
    "    X_test_all_norm = scaler_tgt.transform(X_test_all_raw)\n",
    "    \n",
    "    tensor_test_x = torch.FloatTensor(X_test_all_norm).to(GLOBAL_CONFIG['device'])\n",
    "    tensor_test_y = torch.FloatTensor(y_test_all_raw).to(GLOBAL_CONFIG['device'])\n",
    "    loader_test_val = DataLoader(TensorDataset(tensor_test_x, tensor_test_y), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # 3.3 模型\n",
    "    # 注意这里传入 GLOBAL_CONFIG['input_dim'] (14)\n",
    "    model = SAE_FCN_Paper(input_dim=GLOBAL_CONFIG['input_dim'], latent_dim=GLOBAL_CONFIG['latent_dim']).to(GLOBAL_CONFIG['device'])\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    # 3.4 源域预训练\n",
    "    optimizer_pre = optim.Adam(model.parameters(), lr=lr_pre)\n",
    "    model.train()\n",
    "    # 简单跑50轮预训练\n",
    "    for epoch in range(50): \n",
    "        for bx, by in loader_src:\n",
    "            bx, by = bx.to(GLOBAL_CONFIG['device']), by.to(GLOBAL_CONFIG['device'])\n",
    "            optimizer_pre.zero_grad()\n",
    "            xr, sp, _ = model(bx)\n",
    "            loss = lambda_w * loss_fn(xr, bx) + (1-lambda_w) * loss_fn(sp, by)\n",
    "            loss.backward()\n",
    "            optimizer_pre.step()\n",
    "            \n",
    "    # 3.5 目标域微调 + 早停 (冻结FCN)\n",
    "    for param in model.fcn.parameters(): param.requires_grad = False\n",
    "    \n",
    "    optimizer_fine = optim.Adam(list(model.enc1.parameters()) + list(model.enc2.parameters()) + \\\n",
    "                                list(model.dec1.parameters()) + list(model.dec2.parameters()), lr=lr_fine)\n",
    "    \n",
    "    es = EarlyStopping(patience=GLOBAL_CONFIG['patience'])\n",
    "    \n",
    "    for epoch in range(GLOBAL_CONFIG['max_epochs']):\n",
    "        # Train\n",
    "        model.train()\n",
    "        for bx, by in loader_tgt_tr:\n",
    "            bx, by = bx.to(GLOBAL_CONFIG['device']), by.to(GLOBAL_CONFIG['device'])\n",
    "            optimizer_fine.zero_grad()\n",
    "            xr, sp, _ = model(bx)\n",
    "            loss = lambda_w * loss_fn(xr, bx) + (1-lambda_w) * loss_fn(sp, by)\n",
    "            loss.backward()\n",
    "            optimizer_fine.step()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for bx, by in loader_test_val:\n",
    "                xr, sp, _ = model(bx)\n",
    "                l = lambda_w * loss_fn(xr, bx) + (1-lambda_w) * loss_fn(sp, by)\n",
    "                val_loss += l.item()\n",
    "        val_loss /= len(loader_test_val)\n",
    "        \n",
    "        es(val_loss, model)\n",
    "        if es.early_stop: break\n",
    "            \n",
    "        trial.report(val_loss, epoch)\n",
    "        if trial.should_prune(): raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    # =========================================================\n",
    "    # 3.6 评估与保存\n",
    "    # =========================================================\n",
    "    \n",
    "    # 获取最佳模型参数\n",
    "    model.load_state_dict(es.best_model_state)\n",
    "    model.eval()\n",
    "    \n",
    "    # --- 1. 创建当次Trial的子文件夹 ---\n",
    "    trial_subfolder = os.path.join(GLOBAL_CONFIG['save_dir'], f\"Trial_{trial.number}\")\n",
    "    os.makedirs(trial_subfolder, exist_ok=True)\n",
    "    \n",
    "    # --- 2. 保存模型权重和完整模型 ---\n",
    "    torch.save(model.state_dict(), os.path.join(trial_subfolder, \"model_weights.pth\"))\n",
    "    torch.save(model, os.path.join(trial_subfolder, \"full_model.pth\"))\n",
    "    \n",
    "    # 准备Excel写入器\n",
    "    excel_path = os.path.join(trial_subfolder, \"Predictions.xlsx\")\n",
    "    writer = pd.ExcelWriter(excel_path, engine='openpyxl')\n",
    "    \n",
    "    # 用于收集所有数据以计算 Overall Metrics\n",
    "    all_trues_list, all_preds_list = [], []\n",
    "    \n",
    "    # 用于收集每辆车的 Metrics 写入 TXT\n",
    "    metrics_report_lines = []\n",
    "    metrics_report_lines.append(f\"=== Trial {trial.number} Detailed Report ===\\n\")\n",
    "    metrics_report_lines.append(f\"Hyperparameters: {json.dumps(trial.params, indent=4)}\\n\")\n",
    "    metrics_report_lines.append(\"-\" * 50 + \"\\n\")\n",
    "    metrics_report_lines.append(f\"{'Car Name':<20} | {'RMSE(%)':<10} | {'MAE(%)':<10} | {'R2':<10} | {'MaxError(%)':<10}\\n\")\n",
    "    metrics_report_lines.append(\"-\" * 80 + \"\\n\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, car_info in enumerate(test_cars_data):\n",
    "            # 预测\n",
    "            x_norm = scaler_tgt.transform(car_info['X_raw'])\n",
    "            t_x = torch.FloatTensor(x_norm).to(GLOBAL_CONFIG['device'])\n",
    "            _, pred_01, _ = model(t_x)\n",
    "            \n",
    "            # 转为 numpy 并还原百分比\n",
    "            # 这里SOH本身是[0,1]区间，乘以100变成百分数用于指标计算\n",
    "            pred_01 = pred_01.cpu().numpy().flatten()\n",
    "            true_01 = car_info['y_raw'].flatten()\n",
    "            pred_pct = pred_01 * 100\n",
    "            true_pct = true_01 * 100\n",
    "            \n",
    "            # 收集该车 metrics\n",
    "            m_rmse = np.sqrt(mean_squared_error(true_pct, pred_pct))\n",
    "            m_mae = mean_absolute_error(true_pct, pred_pct)\n",
    "            m_r2 = r2_score(true_pct, pred_pct)\n",
    "            m_max = max_error(true_pct, pred_pct)\n",
    "            \n",
    "            # 格式化并添加到报告列表\n",
    "            car_name_short = car_info['name'].replace('.csv', '')\n",
    "            metrics_report_lines.append(f\"{car_name_short[:19]:<20} | {m_rmse:<10.4f} | {m_mae:<10.4f} | {m_r2:<10.4f} | {m_max:<10.4f}\\n\")\n",
    "            \n",
    "            # 添加到汇总列表\n",
    "            all_trues_list.extend(true_pct)\n",
    "            all_preds_list.extend(pred_pct)\n",
    "            \n",
    "            # --- 3. 写入Excel ---\n",
    "            # 为了避免Excel Sheet名字过长报错，截取前30字符\n",
    "            pd.DataFrame({\n",
    "                'start_time': car_info['times'],\n",
    "                'True_SOH': true_01,      # Excel里保存原始小数\n",
    "                'Predicted_SOH': pred_01, # Excel里保存原始小数\n",
    "                'True_SOH_Pct': true_pct, # 同时保存百分比方便查看\n",
    "                'Predicted_SOH_Pct': pred_pct\n",
    "            }).to_excel(writer, sheet_name=car_name_short[:30], index=False)\n",
    "            \n",
    "    writer.close()\n",
    "    \n",
    "    # 计算 Overall Metrics\n",
    "    all_trues = np.array(all_trues_list)\n",
    "    all_preds = np.array(all_preds_list)\n",
    "    \n",
    "    ov_rmse = np.sqrt(mean_squared_error(all_trues, all_preds))\n",
    "    ov_mae = mean_absolute_error(all_trues, all_preds)\n",
    "    ov_r2 = r2_score(all_trues, all_preds)\n",
    "    ov_max = max_error(all_trues, all_preds)\n",
    "    \n",
    "    # 补充报告底部\n",
    "    metrics_report_lines.append(\"-\" * 80 + \"\\n\")\n",
    "    metrics_report_lines.append(f\"{'OVERALL (Combined)':<20} | {ov_rmse:<10.4f} | {ov_mae:<10.4f} | {ov_r2:<10.4f} | {ov_max:<10.4f}\\n\")\n",
    "    \n",
    "    # --- 4. 保存 TXT 报告 ---\n",
    "    txt_path = os.path.join(trial_subfolder, \"hyperparameters_metrics.txt\")\n",
    "    with open(txt_path, 'w', encoding='utf-8') as f:\n",
    "        f.writelines(metrics_report_lines)\n",
    "    \n",
    "    print(f\"[Trial {trial.number}] RMSE: {ov_rmse:.4f}% | Saved to folder: {os.path.basename(trial_subfolder)}\")\n",
    "    return ov_rmse\n",
    "\n",
    "# ==========================================\n",
    "# 4. 主流程\n",
    "# ==========================================\n",
    "if __name__ == '__main__':\n",
    "    # 4.1 加载源域\n",
    "    print(\">>> 1. Loading Source Domain (Specified CSVs)...\")\n",
    "    X_src, y_src = load_source_data_csv(\n",
    "        GLOBAL_CONFIG['source_files'], \n",
    "        GLOBAL_CONFIG['features'], \n",
    "        GLOBAL_CONFIG['label_col']\n",
    "    )\n",
    "    # 归一化：建议源域和目标域统一使用MinMaxScaler\n",
    "    scaler_src = MinMaxScaler()\n",
    "    X_src_norm = scaler_src.fit_transform(X_src)\n",
    "    \n",
    "    # 4.2 加载目标域训练集 (4个微调电池)\n",
    "    print(\"\\n>>> 2. Loading Target Train Set (Fine-tuning Cars)...\")\n",
    "    X_tgt_tr, y_tgt_tr = load_target_train_stacked_csv(\n",
    "        GLOBAL_CONFIG['target_parent_folder'], \n",
    "        GLOBAL_CONFIG['train_car_names'], \n",
    "        GLOBAL_CONFIG['features'],\n",
    "        GLOBAL_CONFIG['label_col']\n",
    "    )\n",
    "    # 目标域Scaler只在训练集上Fit\n",
    "    scaler_tgt = MinMaxScaler()\n",
    "    X_tgt_tr_norm = scaler_tgt.fit_transform(X_tgt_tr)\n",
    "    \n",
    "    # 4.3 加载目标域测试集 (4个测试电池)\n",
    "    print(\"\\n>>> 3. Loading Target Test Set (Testing Cars)...\")\n",
    "    test_cars_data = load_target_test_individual_csv(\n",
    "        GLOBAL_CONFIG['target_parent_folder'], \n",
    "        GLOBAL_CONFIG['test_car_names'],\n",
    "        GLOBAL_CONFIG['features'],\n",
    "        GLOBAL_CONFIG['label_col']\n",
    "    )\n",
    "    \n",
    "    # 4.4 开启贝叶斯优化\n",
    "    print(f\"\\n>>> 4. Starting Optuna ({GLOBAL_CONFIG['n_trials']} trials)...\")\n",
    "    # 设置Optuna日志级别\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    \n",
    "    func = lambda trial: objective(\n",
    "        trial, \n",
    "        X_src_norm, y_src, \n",
    "        X_tgt_tr_norm, y_tgt_tr,\n",
    "        test_cars_data,\n",
    "        scaler_tgt\n",
    "    )\n",
    "    \n",
    "    study.optimize(func, n_trials=GLOBAL_CONFIG['n_trials'])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"OPTIMIZATION FINISHED\")\n",
    "    print(f\"Best Trial: {study.best_trial.number}\")\n",
    "    print(f\"Best RMSE : {study.best_value:.4f}%\")\n",
    "    print(f\"All trial results saved to: {GLOBAL_CONFIG['save_dir']}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 4.5 绘制最佳结果\n",
    "    try:\n",
    "        best_folder = os.path.join(GLOBAL_CONFIG['save_dir'], f\"Trial_{study.best_trial.number}\")\n",
    "        best_file = os.path.join(best_folder, \"Predictions.xlsx\")\n",
    "        \n",
    "        print(f\"Plotting results from: {best_file}\")\n",
    "        \n",
    "        all_true, all_pred = [], []\n",
    "        xl = pd.ExcelFile(best_file)\n",
    "        for sheet in xl.sheet_names:\n",
    "            df = xl.parse(sheet)\n",
    "            all_true.extend(df['True_SOH_Pct'].values)\n",
    "            all_pred.extend(df['Predicted_SOH_Pct'].values)\n",
    "        \n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.scatter(all_true, all_pred, alpha=0.5, label='Datapoints')\n",
    "        mi, ma = min(min(all_true), min(all_pred)), max(max(all_true), max(all_pred))\n",
    "        plt.plot([mi, ma], [mi, ma], 'r--', label='Ideal')\n",
    "        plt.title(f\"Best Transfer Result (Trial {study.best_trial.number})\\nRMSE: {study.best_value:.4f}%\")\n",
    "        plt.xlabel(\"True SOH (%)\"); plt.ylabel(\"Predicted SOH (%)\")\n",
    "        plt.legend(); plt.grid(True)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Plotting failed: {e}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Results will be saved to: C:\\Users\\CHX\\xwechat_files\\wxid_uomr32ya9e9k11_d15b\\msg\\file\\2026-01\\2409贝叶斯迁移结果\\LFP降采样SOH\n",
      ">>> 1. Loading Source Domain (Specified CSVs)...\n",
      "Loading Source Data from 3 specific files...\n",
      "  Loaded Filtered_Dataset_1_NCA_battery_features_wide.csv: shape (13944, 14)\n",
      "  Loaded Filtered_Dataset_2_NCM_battery_features_wide.csv: shape (17470, 14)\n",
      "  Loaded Filtered_Dataset_3_NCM_NCA_battery_features_wide_downsampled_120s.csv: shape (4118, 14)\n",
      "\n",
      ">>> 2. Loading Target Train Set (Fine-tuning Cars)...\n",
      "Loading Target Train Data: Found 3 files matching ['battery_1', 'battery_5', 'battery_8']\n",
      "  Loaded Train File: Resampled_battery_1_merged_down_120s_14pts.csv\n",
      "  Loaded Train File: Resampled_battery_5_merged_down_120s_14pts.csv\n",
      "  Loaded Train File: Resampled_battery_8_merged_down_120s_14pts.csv\n",
      "\n",
      ">>> 3. Loading Target Test Set (Testing Cars)...\n",
      "Loading Target Test Data: Found 5 files matching ['battery_2', 'battery_4', 'battery_3', 'battery_6', 'battery_7']\n",
      "  Loaded Test File: Resampled_battery_2_merged_down_120s_14pts.csv\n",
      "  Loaded Test File: Resampled_battery_3_merged_down_120s_14pts.csv\n",
      "  Loaded Test File: Resampled_battery_4_merged_down_120s_14pts.csv\n",
      "  Loaded Test File: Resampled_battery_6_merged_down_120s_14pts.csv\n",
      "  Loaded Test File: Resampled_battery_7_merged_down_120s_14pts.csv\n",
      "\n",
      ">>> 4. Starting Optuna (80 trials)...\n",
      "[Trial 0] RMSE: 1.8354% | Saved to folder: Trial_0\n",
      "[Trial 1] RMSE: 1.5545% | Saved to folder: Trial_1\n",
      "[Trial 2] RMSE: 1.8242% | Saved to folder: Trial_2\n",
      "[Trial 3] RMSE: 1.4637% | Saved to folder: Trial_3\n",
      "[Trial 4] RMSE: 1.4174% | Saved to folder: Trial_4\n",
      "[Trial 6] RMSE: 1.3939% | Saved to folder: Trial_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2026-01-04 12:25:01,752] Trial 7 failed with parameters: {'lr_pretrain': 0.0006774174796239654, 'lr_finetune': 0.0005616579343112419, 'lambda_weight': 0.29242765516310987, 'batch_size': 128} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\CHX\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\CHX\\AppData\\Local\\Temp\\ipykernel_38028\\530030776.py\", line 438, in <lambda>\n",
      "    func = lambda trial: objective(\n",
      "  File \"C:\\Users\\CHX\\AppData\\Local\\Temp\\ipykernel_38028\\530030776.py\", line 292, in objective\n",
      "    for bx, by in loader_test_val:\n",
      "  File \"D:\\Anaconda\\envs\\CHX\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 733, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"D:\\Anaconda\\envs\\CHX\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 789, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"D:\\Anaconda\\envs\\CHX\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 55, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"D:\\Anaconda\\envs\\CHX\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 398, in default_collate\n",
      "    return collate(batch, collate_fn_map=default_collate_fn_map)\n",
      "  File \"D:\\Anaconda\\envs\\CHX\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 211, in collate\n",
      "    return [\n",
      "  File \"D:\\Anaconda\\envs\\CHX\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 212, in <listcomp>\n",
      "    collate(samples, collate_fn_map=collate_fn_map)\n",
      "  File \"D:\\Anaconda\\envs\\CHX\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 155, in collate\n",
      "    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n",
      "  File \"D:\\Anaconda\\envs\\CHX\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 272, in collate_tensor_fn\n",
      "    return torch.stack(batch, 0, out=out)\n",
      "KeyboardInterrupt\n",
      "[W 2026-01-04 12:25:01,754] Trial 7 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 446\u001B[0m\n\u001B[0;32m    436\u001B[0m study \u001B[38;5;241m=\u001B[39m optuna\u001B[38;5;241m.\u001B[39mcreate_study(direction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mminimize\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    438\u001B[0m func \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m trial: objective(\n\u001B[0;32m    439\u001B[0m     trial, \n\u001B[0;32m    440\u001B[0m     X_src_norm, y_src, \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    443\u001B[0m     scaler_tgt\n\u001B[0;32m    444\u001B[0m )\n\u001B[1;32m--> 446\u001B[0m \u001B[43mstudy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mGLOBAL_CONFIG\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mn_trials\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    448\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m50\u001B[39m)\n\u001B[0;32m    449\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOPTIMIZATION FINISHED\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\CHX\\lib\\site-packages\\optuna\\study\\study.py:490\u001B[0m, in \u001B[0;36mStudy.optimize\u001B[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[0;32m    388\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21moptimize\u001B[39m(\n\u001B[0;32m    389\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    390\u001B[0m     func: ObjectiveFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    397\u001B[0m     show_progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    398\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    399\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Optimize an objective function.\u001B[39;00m\n\u001B[0;32m    400\u001B[0m \n\u001B[0;32m    401\u001B[0m \u001B[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    488\u001B[0m \u001B[38;5;124;03m            If nested invocation of this method occurs.\u001B[39;00m\n\u001B[0;32m    489\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 490\u001B[0m     \u001B[43m_optimize\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    491\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstudy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    492\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    493\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    494\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    495\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    496\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mIterable\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    497\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshow_progress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshow_progress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    500\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\CHX\\lib\\site-packages\\optuna\\study\\_optimize.py:67\u001B[0m, in \u001B[0;36m_optimize\u001B[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     66\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m---> 67\u001B[0m         \u001B[43m_optimize_sequential\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     68\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     69\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     70\u001B[0m \u001B[43m            \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     71\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     72\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     73\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     74\u001B[0m \u001B[43m            \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     75\u001B[0m \u001B[43m            \u001B[49m\u001B[43mreseed_sampler_rng\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     76\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtime_start\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     77\u001B[0m \u001B[43m            \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     78\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     79\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     80\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\CHX\\lib\\site-packages\\optuna\\study\\_optimize.py:164\u001B[0m, in \u001B[0;36m_optimize_sequential\u001B[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[0m\n\u001B[0;32m    161\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 164\u001B[0m     frozen_trial_id \u001B[38;5;241m=\u001B[39m \u001B[43m_run_trial\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    166\u001B[0m     \u001B[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001B[39;00m\n\u001B[0;32m    167\u001B[0m     \u001B[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001B[39;00m\n\u001B[0;32m    168\u001B[0m     \u001B[38;5;66;03m# Please refer to the following PR for further details:\u001B[39;00m\n\u001B[0;32m    169\u001B[0m     \u001B[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001B[39;00m\n\u001B[0;32m    170\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gc_after_trial:\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\CHX\\lib\\site-packages\\optuna\\study\\_optimize.py:262\u001B[0m, in \u001B[0;36m_run_trial\u001B[1;34m(study, func, catch)\u001B[0m\n\u001B[0;32m    255\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShould not reach.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    257\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    258\u001B[0m     updated_state \u001B[38;5;241m==\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mFAIL\n\u001B[0;32m    259\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m func_err \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    260\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(func_err, catch)\n\u001B[0;32m    261\u001B[0m ):\n\u001B[1;32m--> 262\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m func_err\n\u001B[0;32m    263\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m trial\u001B[38;5;241m.\u001B[39m_trial_id\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\CHX\\lib\\site-packages\\optuna\\study\\_optimize.py:205\u001B[0m, in \u001B[0;36m_run_trial\u001B[1;34m(study, func, catch)\u001B[0m\n\u001B[0;32m    203\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m get_heartbeat_thread(trial\u001B[38;5;241m.\u001B[39m_trial_id, study\u001B[38;5;241m.\u001B[39m_storage):\n\u001B[0;32m    204\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 205\u001B[0m         value_or_values \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    206\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m exceptions\u001B[38;5;241m.\u001B[39mTrialPruned \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    207\u001B[0m         \u001B[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001B[39;00m\n\u001B[0;32m    208\u001B[0m         state \u001B[38;5;241m=\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mPRUNED\n",
      "Cell \u001B[1;32mIn[8], line 438\u001B[0m, in \u001B[0;36m<lambda>\u001B[1;34m(trial)\u001B[0m\n\u001B[0;32m    435\u001B[0m optuna\u001B[38;5;241m.\u001B[39mlogging\u001B[38;5;241m.\u001B[39mset_verbosity(optuna\u001B[38;5;241m.\u001B[39mlogging\u001B[38;5;241m.\u001B[39mWARNING)\n\u001B[0;32m    436\u001B[0m study \u001B[38;5;241m=\u001B[39m optuna\u001B[38;5;241m.\u001B[39mcreate_study(direction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mminimize\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 438\u001B[0m func \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m trial: \u001B[43mobjective\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    439\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m    440\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_src_norm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_src\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m    441\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_tgt_tr_norm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_tgt_tr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    442\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtest_cars_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    443\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscaler_tgt\u001B[49m\n\u001B[0;32m    444\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    446\u001B[0m study\u001B[38;5;241m.\u001B[39moptimize(func, n_trials\u001B[38;5;241m=\u001B[39mGLOBAL_CONFIG[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_trials\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m    448\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m50\u001B[39m)\n",
      "Cell \u001B[1;32mIn[8], line 292\u001B[0m, in \u001B[0;36mobjective\u001B[1;34m(trial, X_src, y_src, X_tgt_train, y_tgt_train, test_cars_data, scaler_tgt)\u001B[0m\n\u001B[0;32m    290\u001B[0m val_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m    291\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 292\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m bx, by \u001B[38;5;129;01min\u001B[39;00m loader_test_val:\n\u001B[0;32m    293\u001B[0m         xr, sp, _ \u001B[38;5;241m=\u001B[39m model(bx)\n\u001B[0;32m    294\u001B[0m         l \u001B[38;5;241m=\u001B[39m lambda_w \u001B[38;5;241m*\u001B[39m loss_fn(xr, bx) \u001B[38;5;241m+\u001B[39m (\u001B[38;5;241m1\u001B[39m\u001B[38;5;241m-\u001B[39mlambda_w) \u001B[38;5;241m*\u001B[39m loss_fn(sp, by)\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\CHX\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    730\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    731\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    732\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 733\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    734\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    735\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    736\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[0;32m    737\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    738\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[0;32m    739\u001B[0m ):\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\CHX\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    787\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    788\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 789\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    790\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    791\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\CHX\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 55\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\CHX\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001B[0m, in \u001B[0;36mdefault_collate\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m    337\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[0;32m    338\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    339\u001B[0m \u001B[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001B[39;00m\n\u001B[0;32m    340\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    396\u001B[0m \u001B[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[0;32m    397\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 398\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_collate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\CHX\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:211\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    208\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m--> 211\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[0;32m    212\u001B[0m         collate(samples, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map)\n\u001B[0;32m    213\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed\n\u001B[0;32m    214\u001B[0m     ]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\CHX\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    208\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    211\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[1;32m--> 212\u001B[0m         \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    213\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed\n\u001B[0;32m    214\u001B[0m     ]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\CHX\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    153\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m collate_fn_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    154\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m elem_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[1;32m--> 155\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate_fn_map\u001B[49m\u001B[43m[\u001B[49m\u001B[43melem_type\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    157\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m collate_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[0;32m    158\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collate_type):\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\CHX\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001B[0m, in \u001B[0;36mcollate_tensor_fn\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    270\u001B[0m     storage \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39m_typed_storage()\u001B[38;5;241m.\u001B[39m_new_shared(numel, device\u001B[38;5;241m=\u001B[39melem\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m    271\u001B[0m     out \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39mnew(storage)\u001B[38;5;241m.\u001B[39mresize_(\u001B[38;5;28mlen\u001B[39m(batch), \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlist\u001B[39m(elem\u001B[38;5;241m.\u001B[39msize()))\n\u001B[1;32m--> 272\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a050741bdcea0a12"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
